{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_title_tags = b1.findAll('a', attrs={'class':'a-link-normal s-access-detail-page a-text-normal'})\n",
    "        ds_title_strings = map(lambda item: item.text , ds_pg_one_title_tags)\n",
    "        title_pattern = re.compile(\".+\")\n",
    "        data_science_titles = map(lambda item: re.search(title_pattern, item).group(0), ds_title_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_title_tags = b2.findAll('a', attrs={'class':'a-link-normal s-access-detail-page a-text-normal'})\n",
    "        ds_title_strings_two = map(lambda item: item.text , ds_pg_two_title_tags)\n",
    "        title_pattern = re.compile(\".+\")\n",
    "        data_science_titles_two = map(lambda item: re.search(title_pattern, item).group(0), ds_title_strings_two)\n",
    "        data_science_book_titles_one_and_two = np.concatenate((data_science_titles, data_science_titles_two))\n",
    "        ds_titles_df = pd.DataFrame(data_science_book_titles_one_and_two)\n",
    "        ds_titles_df.columns = ['Data_Science_Book_Titles']\n",
    "        return ds_titles_df\n",
    "    else:\n",
    "        print ('You Lose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data_Science_Book_Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science from Scratch: First Principles wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science for Business: What You Need to Kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Is Data Science?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science For Dummies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analytics : The Complete Beginner's Guide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naked Statistics: Stripping the Dread from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Doing Data Science: Straight Talk from the Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Smart: Using Data Science to Transform In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Python Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Driven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analytics Made Accessible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Big Data Science &amp; Analytics: A Hands-On Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Storytelling with Data: A Data Visualization G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How Data Science Is Transforming Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How To Start a Career in Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>R for Data Science: Visualize, Model, Transfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Data Science Handbook: Advice and Insights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Machine Learning: The Art and Science of Algor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Introduction to Machine Learning with Python: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fundamentals of Machine Learning for Predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Python Data Science Handbook: Essential Tools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Art of Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Practical Data Science with R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Science and Big Data Analytics: Discoveri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Building Data Science Teams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Python for Data Analysis: Data Wrangling with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Science Essentials in Python: Collect - O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Introducing Data Science: Big Data, Machine Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Analytics for Beginners: Basic Guide to M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Real-World Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Python for Data Science For Dummies (For Dummi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Blittzen Mens T-shirt Two Kinds of People - In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Learning Probabilistic Graphical Models in R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Learning Predictive Analytics with R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Data_Science_Book_Titles\n",
       "0   Data Science from Scratch: First Principles wi...\n",
       "1   Data Science for Business: What You Need to Kn...\n",
       "2                               What Is Data Science?\n",
       "3                            Data Science For Dummies\n",
       "4   Data Analytics : The Complete Beginner's Guide...\n",
       "5   Naked Statistics: Stripping the Dread from the...\n",
       "6   Doing Data Science: Straight Talk from the Fro...\n",
       "7   Data Smart: Using Data Science to Transform In...\n",
       "8                             Python Machine Learning\n",
       "9                                         Data Driven\n",
       "10                     Data Analytics Made Accessible\n",
       "11  Big Data Science & Analytics: A Hands-On Approach\n",
       "12  Storytelling with Data: A Data Visualization G...\n",
       "13       How Data Science Is Transforming Health Care\n",
       "14              How To Start a Career in Data Science\n",
       "15  R for Data Science: Visualize, Model, Transfor...\n",
       "16  The Data Science Handbook: Advice and Insights...\n",
       "17  Machine Learning: The Art and Science of Algor...\n",
       "18  Introduction to Machine Learning with Python: ...\n",
       "19  Fundamentals of Machine Learning for Predictiv...\n",
       "20  Python Data Science Handbook: Essential Tools ...\n",
       "21                            The Art of Data Science\n",
       "22                      Practical Data Science with R\n",
       "23  Data Science and Big Data Analytics: Discoveri...\n",
       "24                        Building Data Science Teams\n",
       "25  Python for Data Analysis: Data Wrangling with ...\n",
       "26  Data Science Essentials in Python: Collect - O...\n",
       "27  Introducing Data Science: Big Data, Machine Le...\n",
       "28  Data Analytics for Beginners: Basic Guide to M...\n",
       "29                        Real-World Machine Learning\n",
       "30  Python for Data Science For Dummies (For Dummi...\n",
       "31  Blittzen Mens T-shirt Two Kinds of People - In...\n",
       "32       Learning Probabilistic Graphical Models in R\n",
       "33               Learning Predictive Analytics with R"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "class Book:\n",
    "    def __init__(self, title, url):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        \n",
    "#class BookReferenceFinder answers a collection of references to books in a given category\n",
    "class BookReferenceFinder:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "\n",
    "    def rootUrl(self):\n",
    "        return 'https://www.amazon.com'\n",
    "    \n",
    "    def url(self):\n",
    "        return self.rootUrl()+'/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords='+(self.category.replace(' ', '+'))\n",
    "\n",
    "    def bookReferences(self):\n",
    "        return self.bookReferencesOnPages(self.url(),None)\n",
    "    \n",
    "    def maxPagesOfBookReferences(self,n):\n",
    "        return self.bookReferencesOnPages(self.url(),n)\n",
    "\n",
    "    def bookReferencesOnPages(self,pageurl,max):\n",
    "        page = requests.get(pageurl, headers=headers)\n",
    "        parsedPage = BeautifulSoup(page.text, 'html.parser')\n",
    "        title_tags = parsedPage.findAll('a', attrs={'class':'a-link-normal s-access-detail-page a-text-normal'})\n",
    "        next_link = parsedPage.find('a', attrs={'class':'pagnNext'})\n",
    "        if next_link == None:\n",
    "            return title_tags\n",
    "        if max != None:\n",
    "            max = max - 1\n",
    "            if max == 0:\n",
    "                return title_tags\n",
    "        #print (next_link.get('href').replace('&amp;','&'))\n",
    "        return title_tags + self.bookReferencesOnPages(self.rootUrl()+next_link.get('href').replace('&amp;','&'),max)\n",
    "\n",
    "class BookProfileFactory:\n",
    "    def __init__(self, category, maxPages):\n",
    "        self.category = category\n",
    "        self.maxPages = maxPages\n",
    "        \n",
    "    def bookFromRefUrl(self,bookref):\n",
    "        price = bookref.find('span', attrs={'class':'a-size-base a-color-price s-price a-text-bold'})\n",
    "        rating = bookref.find('span', attrs={'class':'a-icon-alt'})\n",
    "        return [price,rating]\n",
    "         \n",
    "\n",
    "    def books(self):\n",
    "        bookFinder = BookReferenceFinder('data science')\n",
    "        return map(lambda bookref: self.bookFromRefUrl(bookref),\n",
    "                   bookFinder.maxPagesOfBookReferences(self.maxPages))\n",
    "\n",
    "BookProfileFactory('data science',2).bookFromRefUrl(\n",
    "    (BookReferenceFinder('data science').maxPagesOfBookReferences(1))[1])\n",
    "                   \n",
    "#(BookReferenceFinder('data science').maxPagesOfBookReferences(2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-74657e6ff666>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-74657e6ff666>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def geturl(topic.lower()):\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def rating(topic.lower()):\n",
    "    if topic.lower() == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return ds_df\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        da_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings = map(lambda item: item.text , da_pg_one_rating_tags)\n",
    "        da_user_rating_strings = filter(lambda item: 'out of' in item, da_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478123303&spIA=1784390860,B019N212NE'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+analysis&page=2&keywords=data+analysis'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        da_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings_two = map(lambda item: item.text , da_pg_two_rating_tags)\n",
    "        da_user_rating_strings_two = filter(lambda item: 'out of' in item, da_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings_two)\n",
    "        data_analysis_reviews_one_and_two = np.concatenate((data_analysis_reviews, data_analysis_reviews_two))\n",
    "        da_df = pd.DataFrame(data_analysis_reviews_one_and_two)\n",
    "        da_df.columns = ['Data_Analysis_User_Reviews']\n",
    "        return da_df\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dv_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings = map(lambda item: item.text , dv_pg_one_rating_tags)\n",
    "        dv_user_rating_strings = filter(lambda item: 'out of' in item, dv_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053109&spIA=1784395803,1784391603'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+visualization&page=2&keywords=data+visualization'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dv_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings_two = map(lambda item: item.text , dv_pg_two_rating_tags)\n",
    "        dv_user_rating_strings_two = filter(lambda item: 'out of' in item, dv_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings_two)\n",
    "        data_viz_reviews_one_and_two = np.concatenate((data_viz_reviews, data_viz_reviews_two))\n",
    "        dv_df = pd.DataFrame(data_viz_reviews_one_and_two)\n",
    "        dv_df.columns = ['Data_Viz_User_Reviews']\n",
    "        return dv_df\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dm_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings = map(lambda item: item.text , dm_pg_one_rating_tags)\n",
    "        dm_user_rating_strings = filter(lambda item: 'out of' in item, dm_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053297'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+management&page=2&keywords=data+management'+page,headers=headers)  \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dm_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings_two = map(lambda item: item.text , dm_pg_two_rating_tags)\n",
    "        dm_user_rating_strings_two = filter(lambda item: 'out of' in item, dm_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings_two)\n",
    "        data_mgmt_reviews_one_and_two = np.concatenate((data_mgmt_reviews, data_mgmt_reviews_two))\n",
    "        dm_df = pd.DataFrame(data_mgmt_reviews_one_and_two)\n",
    "        dm_df.columns = ['Data_Mgmt_User_Reviews']\n",
    "        return dm_df\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        py_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings = map(lambda item: item.text , py_pg_one_rating_tags)\n",
    "        py_user_rating_strings = filter(lambda item: 'out of' in item, py_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053333&spIA=B01M63XMN1,B00WFP9S2E'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Apython&page=2&keywords=python'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        py_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings_two = map(lambda item: item.text , py_pg_two_rating_tags)\n",
    "        py_user_rating_strings_two = filter(lambda item: 'out of' in item, py_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings_two)\n",
    "        python_reviews_one_and_two = np.concatenate((python_reviews, python_reviews_two))\n",
    "        py_df = pd.DataFrame(python_reviews_one_and_two)\n",
    "        py_df.columns = ['Python_User_Reviews']\n",
    "        return py_df\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        r_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings = map(lambda item: item.text , r_pg_one_rating_tags)\n",
    "        r_user_rating_strings = filter(lambda item: 'out of' in item, r_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053380&spIA=1785283529,1784391034'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Ar+programming&page=2&keywords=r+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        r_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings_two = map(lambda item: item.text , r_pg_two_rating_tags)\n",
    "        r_user_rating_strings_two = filter(lambda item: 'out of' in item, r_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings_two)\n",
    "        r_reviews_one_and_two = np.concatenate((r_reviews, r_reviews_two))\n",
    "        r_df = pd.DataFrame(r_reviews_one_and_two)\n",
    "        r_df.columns = ['R_User_Reviews']\n",
    "        return r_df\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sas_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings = map(lambda item: item.text , sas_pg_one_rating_tags)\n",
    "        sas_user_rating_strings = filter(lambda item: 'out of' in item, sas_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053426'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asas+programming&page=2&keywords=sas+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sas_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings_two = map(lambda item: item.text , sas_pg_two_rating_tags)\n",
    "        sas_user_rating_strings_two = filter(lambda item: 'out of' in item, sas_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings_two)\n",
    "        sas_reviews_one_and_two = np.concatenate((sas_reviews, sas_reviews_two))\n",
    "        sas_df = pd.DataFrame(sas_reviews_one_and_two)\n",
    "        sas_df.columns = ['SAS_User_Reviews']\n",
    "        return sas_df\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        stata_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings = map(lambda item: item.text , stata_pg_one_rating_tags)\n",
    "        stata_user_rating_strings = filter(lambda item: 'out of' in item, stata_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053462&spIA=1785288148,1783981962'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Astata&page=2&keywords=stata'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        stata_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings_two = map(lambda item: item.text , stata_pg_two_rating_tags)\n",
    "        stata_user_rating_strings_two = filter(lambda item: 'out of' in item, stata_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings_two)\n",
    "        stata_reviews_one_and_two = np.concatenate((stata_reviews, stata_reviews_two))\n",
    "        stata_df = pd.DataFrame(stata_reviews_one_and_two)\n",
    "        stata_df.columns = ['STATA_User_Reviews']\n",
    "        return stata_df\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sql_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings = map(lambda item: item.text , sql_pg_one_rating_tags)\n",
    "        sql_user_rating_strings = filter(lambda item: 'out of' in item, sql_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053498&spIA=B01GK955A4,1782173455'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asql&page=2&keywords=sql'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sql_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings_two = map(lambda item: item.text , sql_pg_two_rating_tags)\n",
    "        sql_user_rating_strings_two = filter(lambda item: 'out of' in item, sql_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings_two)\n",
    "        sql_reviews_one_and_two = np.concatenate((sql_reviews, sql_reviews_two))\n",
    "        sql_df = pd.DataFrame(sql_reviews_one_and_two)\n",
    "        sql_df.columns = ['SQL_User_Reviews']\n",
    "        return sql_df\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        wd_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings = map(lambda item: item.text , wd_pg_one_rating_tags)\n",
    "        wd_user_rating_strings = filter(lambda item: 'out of' in item, wd_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053541&spIA=B01FGXTDGW'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Aweb+development&page=2&keywords=web+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        wd_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings_two = map(lambda item: item.text , wd_pg_two_rating_tags)\n",
    "        wd_user_rating_strings_two = filter(lambda item: 'out of' in item, wd_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings_two)\n",
    "        wd_reviews_one_and_two = np.concatenate((wd_reviews, wd_reviews_two))\n",
    "        wd_df = pd.DataFrame(wd_reviews_one_and_two)\n",
    "        wd_df.columns = ['Web_Dev_User_Reviews']\n",
    "        return wd_df\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        vr_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings = map(lambda item: item.text , vr_pg_one_rating_tags)\n",
    "        vr_user_rating_strings = filter(lambda item: 'out of' in item, vr_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053592&ajr=1'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Avirtual+reality+development&page=2&keywords=virtual+reality+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        vr_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings_two = map(lambda item: item.text , vr_pg_two_rating_tags)\n",
    "        vr_user_rating_strings_two = filter(lambda item: 'out of' in item, vr_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings_two)\n",
    "        vr_reviews_one_and_two = np.concatenate((vr_reviews, vr_reviews_two))\n",
    "        vr_df = pd.DataFrame(vr_reviews_one_and_two)\n",
    "        vr_df.columns = ['VR_User_Reviews']\n",
    "        return vr_df\n",
    "    else:\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        da_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings = map(lambda item: item.text , da_pg_one_rating_tags)\n",
    "        da_user_rating_strings = filter(lambda item: 'out of' in item, da_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478123303&spIA=1784390860,B019N212NE'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+analysis&page=2&keywords=data+analysis'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        da_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings_two = map(lambda item: item.text , da_pg_two_rating_tags)\n",
    "        da_user_rating_strings_two = filter(lambda item: 'out of' in item, da_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings_two)\n",
    "        data_analysis_reviews_one_and_two = np.concatenate((data_analysis_reviews, data_analysis_reviews_two))\n",
    "        da_df = pd.DataFrame(data_analysis_reviews_one_and_two)\n",
    "        da_df.columns = ['Data_Analysis_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dv_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings = map(lambda item: item.text , dv_pg_one_rating_tags)\n",
    "        dv_user_rating_strings = filter(lambda item: 'out of' in item, dv_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053109&spIA=1784395803,1784391603'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+visualization&page=2&keywords=data+visualization'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dv_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings_two = map(lambda item: item.text , dv_pg_two_rating_tags)\n",
    "        dv_user_rating_strings_two = filter(lambda item: 'out of' in item, dv_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings_two)\n",
    "        data_viz_reviews_one_and_two = np.concatenate((data_viz_reviews, data_viz_reviews_two))\n",
    "        dv_df = pd.DataFrame(data_viz_reviews_one_and_two)\n",
    "        dv_df.columns = ['Data_Viz_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dm_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings = map(lambda item: item.text , dm_pg_one_rating_tags)\n",
    "        dm_user_rating_strings = filter(lambda item: 'out of' in item, dm_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053297'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+management&page=2&keywords=data+management'+page,headers=headers)  \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dm_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings_two = map(lambda item: item.text , dm_pg_two_rating_tags)\n",
    "        dm_user_rating_strings_two = filter(lambda item: 'out of' in item, dm_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings_two)\n",
    "        data_mgmt_reviews_one_and_two = np.concatenate((data_mgmt_reviews, data_mgmt_reviews_two))\n",
    "        dm_df = pd.DataFrame(data_mgmt_reviews_one_and_two)\n",
    "        dm_df.columns = ['Data_Mgmt_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        py_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings = map(lambda item: item.text , py_pg_one_rating_tags)\n",
    "        py_user_rating_strings = filter(lambda item: 'out of' in item, py_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053333&spIA=B01M63XMN1,B00WFP9S2E'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Apython&page=2&keywords=python'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        py_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings_two = map(lambda item: item.text , py_pg_two_rating_tags)\n",
    "        py_user_rating_strings_two = filter(lambda item: 'out of' in item, py_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings_two)\n",
    "        python_reviews_one_and_two = np.concatenate((python_reviews, python_reviews_two))\n",
    "        py_df = pd.DataFrame(python_reviews_one_and_two)\n",
    "        py_df.columns = ['Python_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        r_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings = map(lambda item: item.text , r_pg_one_rating_tags)\n",
    "        r_user_rating_strings = filter(lambda item: 'out of' in item, r_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053380&spIA=1785283529,1784391034'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Ar+programming&page=2&keywords=r+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        r_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings_two = map(lambda item: item.text , r_pg_two_rating_tags)\n",
    "        r_user_rating_strings_two = filter(lambda item: 'out of' in item, r_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings_two)\n",
    "        r_reviews_one_and_two = np.concatenate((r_reviews, r_reviews_two))\n",
    "        r_df = pd.DataFrame(r_reviews_one_and_two)\n",
    "        r_df.columns = ['R_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sas_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings = map(lambda item: item.text , sas_pg_one_rating_tags)\n",
    "        sas_user_rating_strings = filter(lambda item: 'out of' in item, sas_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053426'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asas+programming&page=2&keywords=sas+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sas_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings_two = map(lambda item: item.text , sas_pg_two_rating_tags)\n",
    "        sas_user_rating_strings_two = filter(lambda item: 'out of' in item, sas_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings_two)\n",
    "        sas_reviews_one_and_two = np.concatenate((sas_reviews, sas_reviews_two))\n",
    "        sas_df = pd.DataFrame(sas_reviews_one_and_two)\n",
    "        sas_df.columns = ['SAS_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        stata_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings = map(lambda item: item.text , stata_pg_one_rating_tags)\n",
    "        stata_user_rating_strings = filter(lambda item: 'out of' in item, stata_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053462&spIA=1785288148,1783981962'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Astata&page=2&keywords=stata'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        stata_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings_two = map(lambda item: item.text , stata_pg_two_rating_tags)\n",
    "        stata_user_rating_strings_two = filter(lambda item: 'out of' in item, stata_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings_two)\n",
    "        stata_reviews_one_and_two = np.concatenate((stata_reviews, stata_reviews_two))\n",
    "        stata_df = pd.DataFrame(stata_reviews_one_and_two)\n",
    "        stata_df.columns = ['STATA_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sql_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings = map(lambda item: item.text , sql_pg_one_rating_tags)\n",
    "        sql_user_rating_strings = filter(lambda item: 'out of' in item, sql_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053498&spIA=B01GK955A4,1782173455'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asql&page=2&keywords=sql'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sql_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings_two = map(lambda item: item.text , sql_pg_two_rating_tags)\n",
    "        sql_user_rating_strings_two = filter(lambda item: 'out of' in item, sql_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings_two)\n",
    "        sql_reviews_one_and_two = np.concatenate((sql_reviews, sql_reviews_two))\n",
    "        sql_df = pd.DataFrame(sql_reviews_one_and_two)\n",
    "        sql_df.columns = ['SQL_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        wd_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings = map(lambda item: item.text , wd_pg_one_rating_tags)\n",
    "        wd_user_rating_strings = filter(lambda item: 'out of' in item, wd_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053541&spIA=B01FGXTDGW'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Aweb+development&page=2&keywords=web+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        wd_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings_two = map(lambda item: item.text , wd_pg_two_rating_tags)\n",
    "        wd_user_rating_strings_two = filter(lambda item: 'out of' in item, wd_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings_two)\n",
    "        wd_reviews_one_and_two = np.concatenate((wd_reviews, wd_reviews_two))\n",
    "        wd_df = pd.DataFrame(wd_reviews_one_and_two)\n",
    "        wd_df.columns = ['Web_Dev_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        vr_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings = map(lambda item: item.text , vr_pg_one_rating_tags)\n",
    "        vr_user_rating_strings = filter(lambda item: 'out of' in item, vr_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053592&ajr=1'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Avirtual+reality+development&page=2&keywords=virtual+reality+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        vr_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings_two = map(lambda item: item.text , vr_pg_two_rating_tags)\n",
    "        vr_user_rating_strings_two = filter(lambda item: 'out of' in item, vr_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings_two)\n",
    "        vr_reviews_one_and_two = np.concatenate((vr_reviews, vr_reviews_two))\n",
    "        vr_df = pd.DataFrame(vr_reviews_one_and_two)\n",
    "        vr_df.columns = ['VR_User_Reviews']\n",
    "        user_ratings = pd.concat([ds_df, da_df , dv_df, dm_df, py_df, r_df, sas_df, stata_df, sql_df, wd_df, vr_df], axis=1)\n",
    "        print ('If you want a specific subject area, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')\n",
    "        return user_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want a specific subject area, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data_Science_User_Reviews</th>\n",
       "      <th>Data_Analysis_User_Reviews</th>\n",
       "      <th>Data_Viz_User_Reviews</th>\n",
       "      <th>Data_Mgmt_User_Reviews</th>\n",
       "      <th>Python_User_Reviews</th>\n",
       "      <th>R_User_Reviews</th>\n",
       "      <th>SAS_User_Reviews</th>\n",
       "      <th>STATA_User_Reviews</th>\n",
       "      <th>SQL_User_Reviews</th>\n",
       "      <th>Web_Dev_User_Reviews</th>\n",
       "      <th>VR_User_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data_Science_User_Reviews  Data_Analysis_User_Reviews  \\\n",
       "0                         4.2                         4.6   \n",
       "1                         4.6                         4.2   \n",
       "2                         4.2                         4.7   \n",
       "3                         4.7                         5.0   \n",
       "4                         4.3                         4.2   \n",
       "5                         4.4                         4.8   \n",
       "6                         4.0                         4.4   \n",
       "7                         4.7                         4.6   \n",
       "8                         4.3                         4.4   \n",
       "9                         4.3                         4.4   \n",
       "10                        4.8                         3.9   \n",
       "11                        4.6                         4.2   \n",
       "12                        5.0                         4.7   \n",
       "13                        4.3                         4.3   \n",
       "14                        4.2                         4.5   \n",
       "15                        4.1                         3.7   \n",
       "16                        4.0                         3.4   \n",
       "17                        4.3                         5.0   \n",
       "18                        3.5                         3.4   \n",
       "19                        4.3                         4.6   \n",
       "20                        3.5                         4.6   \n",
       "21                        4.6                         3.1   \n",
       "22                        4.7                         4.4   \n",
       "23                        4.7                         3.8   \n",
       "24                        4.3                         4.7   \n",
       "25                        4.3                         4.5   \n",
       "26                        4.2                         4.2   \n",
       "27                        4.2                         3.9   \n",
       "28                        4.8                         4.6   \n",
       "29                        4.0                         4.8   \n",
       "30                        NaN                         3.6   \n",
       "31                        NaN                         NaN   \n",
       "32                        NaN                         NaN   \n",
       "33                        NaN                         NaN   \n",
       "34                        NaN                         NaN   \n",
       "\n",
       "    Data_Viz_User_Reviews  Data_Mgmt_User_Reviews  Python_User_Reviews  \\\n",
       "0                     4.6                     3.8                  4.4   \n",
       "1                     4.5                     4.3                  4.0   \n",
       "2                     4.4                     5.0                  4.6   \n",
       "3                     4.5                     4.8                  4.6   \n",
       "4                     4.6                     4.4                  4.8   \n",
       "5                     4.9                     4.3                  4.5   \n",
       "6                     4.3                     4.0                  4.8   \n",
       "7                     5.0                     4.3                  4.1   \n",
       "8                     4.3                     5.0                  4.4   \n",
       "9                     3.9                     4.5                  4.5   \n",
       "10                    4.3                     5.0                  4.3   \n",
       "11                    4.5                     4.6                  3.5   \n",
       "12                    3.6                     3.6                  4.3   \n",
       "13                    4.4                     4.2                  4.7   \n",
       "14                    4.4                     4.6                  3.8   \n",
       "15                    4.5                     5.0                  4.4   \n",
       "16                    4.7                     5.0                  4.0   \n",
       "17                    4.1                     4.3                  4.7   \n",
       "18                    5.0                     4.6                  4.2   \n",
       "19                    4.3                     5.0                  4.3   \n",
       "20                    4.6                     4.3                  4.2   \n",
       "21                    4.4                     4.5                  4.7   \n",
       "22                    4.5                     3.4                  4.5   \n",
       "23                    3.8                     4.2                  4.7   \n",
       "24                    3.7                     4.5                  4.6   \n",
       "25                    3.0                     3.9                  4.6   \n",
       "26                    4.7                     NaN                  4.9   \n",
       "27                    4.3                     NaN                  4.7   \n",
       "28                    4.0                     NaN                  4.1   \n",
       "29                    5.0                     NaN                  4.4   \n",
       "30                    NaN                     NaN                  4.0   \n",
       "31                    NaN                     NaN                  4.6   \n",
       "32                    NaN                     NaN                  3.9   \n",
       "33                    NaN                     NaN                  NaN   \n",
       "34                    NaN                     NaN                  NaN   \n",
       "\n",
       "    R_User_Reviews  SAS_User_Reviews  STATA_User_Reviews  SQL_User_Reviews  \\\n",
       "0              4.8               4.3                 5.0               4.6   \n",
       "1              4.6               4.3                 4.3               4.4   \n",
       "2              4.8               5.0                 4.2               4.6   \n",
       "3              4.4               4.0                 4.9               4.2   \n",
       "4              3.8               5.0                 5.0               4.6   \n",
       "5              4.3               4.1                 4.9               4.0   \n",
       "6              4.4               3.6                 4.8               4.5   \n",
       "7              4.9               4.2                 5.0               4.3   \n",
       "8              4.2               4.8                 4.5               4.4   \n",
       "9              4.8               3.5                 5.0               4.6   \n",
       "10             4.7               4.5                 4.7               4.2   \n",
       "11             3.9               4.0                 4.2               4.7   \n",
       "12             4.2               5.0                 4.7               4.7   \n",
       "13             3.9               4.2                 4.6               4.4   \n",
       "14             5.0               3.7                 3.0               4.2   \n",
       "15             4.0               5.0                 4.7               4.4   \n",
       "16             4.5               5.0                 4.6               4.5   \n",
       "17             4.6               4.0                 1.5               5.0   \n",
       "18             4.5               4.3                 4.8               4.5   \n",
       "19             3.6               3.0                 4.2               4.1   \n",
       "20             4.4               4.7                 4.1               1.3   \n",
       "21             4.7               3.4                 4.2               5.0   \n",
       "22             5.0               4.0                 4.0               4.2   \n",
       "23             4.1               4.5                 4.8               4.6   \n",
       "24             4.3               4.2                 3.0               4.2   \n",
       "25             5.0               3.0                 3.8               4.3   \n",
       "26             4.3               4.3                 NaN               5.0   \n",
       "27             4.7               4.7                 NaN               4.6   \n",
       "28             4.3               NaN                 NaN               4.0   \n",
       "29             4.3               NaN                 NaN               4.4   \n",
       "30             3.8               NaN                 NaN               4.0   \n",
       "31             3.0               NaN                 NaN               4.6   \n",
       "32             3.7               NaN                 NaN               4.1   \n",
       "33             NaN               NaN                 NaN               4.5   \n",
       "34             NaN               NaN                 NaN               3.7   \n",
       "\n",
       "    Web_Dev_User_Reviews  VR_User_Reviews  \n",
       "0                    4.7              4.3  \n",
       "1                    4.5              4.4  \n",
       "2                    4.6              3.3  \n",
       "3                    4.3              4.0  \n",
       "4                    4.7              4.0  \n",
       "5                    4.4              4.5  \n",
       "6                    4.7              4.5  \n",
       "7                    4.9              5.0  \n",
       "8                    5.0              4.5  \n",
       "9                    4.4              5.0  \n",
       "10                   5.0              4.1  \n",
       "11                   3.8              2.6  \n",
       "12                   4.4              5.0  \n",
       "13                   4.6              4.0  \n",
       "14                   4.7              4.2  \n",
       "15                   4.1              4.3  \n",
       "16                   4.5              4.2  \n",
       "17                   4.1              5.0  \n",
       "18                   4.6              3.0  \n",
       "19                   4.8              NaN  \n",
       "20                   4.2              NaN  \n",
       "21                   4.1              NaN  \n",
       "22                   1.0              NaN  \n",
       "23                   4.6              NaN  \n",
       "24                   3.3              NaN  \n",
       "25                   4.0              NaN  \n",
       "26                   2.8              NaN  \n",
       "27                   4.3              NaN  \n",
       "28                   5.0              NaN  \n",
       "29                   5.0              NaN  \n",
       "30                   3.0              NaN  \n",
       "31                   4.3              NaN  \n",
       "32                   NaN              NaN  \n",
       "33                   NaN              NaN  \n",
       "34                   NaN              NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summaryStats(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return ds_df.Data_Science_User_Reviews.describe()\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        da_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings = map(lambda item: item.text , da_pg_one_rating_tags)\n",
    "        da_user_rating_strings = filter(lambda item: 'out of' in item, da_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478123303&spIA=1784390860,B019N212NE'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+analysis&page=2&keywords=data+analysis'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        da_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings_two = map(lambda item: item.text , da_pg_two_rating_tags)\n",
    "        da_user_rating_strings_two = filter(lambda item: 'out of' in item, da_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings_two)\n",
    "        data_analysis_reviews_one_and_two = np.concatenate((data_analysis_reviews, data_analysis_reviews_two))\n",
    "        da_df = pd.DataFrame(data_analysis_reviews_one_and_two)\n",
    "        da_df.columns = ['Data_Analysis_User_Reviews']\n",
    "        return da_df.Data_Analysis_User_Reviews.describe()\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dv_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings = map(lambda item: item.text , dv_pg_one_rating_tags)\n",
    "        dv_user_rating_strings = filter(lambda item: 'out of' in item, dv_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053109&spIA=1784395803,1784391603'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+visualization&page=2&keywords=data+visualization'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dv_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings_two = map(lambda item: item.text , dv_pg_two_rating_tags)\n",
    "        dv_user_rating_strings_two = filter(lambda item: 'out of' in item, dv_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings_two)\n",
    "        data_viz_reviews_one_and_two = np.concatenate((data_viz_reviews, data_viz_reviews_two))\n",
    "        dv_df = pd.DataFrame(data_viz_reviews_one_and_two)\n",
    "        dv_df.columns = ['Data_Viz_User_Reviews']\n",
    "        return dv_df.Data_Viz_User_Reviews.describe()\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dm_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings = map(lambda item: item.text , dm_pg_one_rating_tags)\n",
    "        dm_user_rating_strings = filter(lambda item: 'out of' in item, dm_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053297'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+management&page=2&keywords=data+management'+page,headers=headers)  \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dm_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings_two = map(lambda item: item.text , dm_pg_two_rating_tags)\n",
    "        dm_user_rating_strings_two = filter(lambda item: 'out of' in item, dm_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings_two)\n",
    "        data_mgmt_reviews_one_and_two = np.concatenate((data_mgmt_reviews, data_mgmt_reviews_two))\n",
    "        dm_df = pd.DataFrame(data_mgmt_reviews_one_and_two)\n",
    "        dm_df.columns = ['Data_Mgmt_User_Reviews']\n",
    "        return dm_df.Data_Mgmt_User_Reviews.describe()\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        py_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings = map(lambda item: item.text , py_pg_one_rating_tags)\n",
    "        py_user_rating_strings = filter(lambda item: 'out of' in item, py_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053333&spIA=B01M63XMN1,B00WFP9S2E'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Apython&page=2&keywords=python'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        py_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings_two = map(lambda item: item.text , py_pg_two_rating_tags)\n",
    "        py_user_rating_strings_two = filter(lambda item: 'out of' in item, py_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings_two)\n",
    "        python_reviews_one_and_two = np.concatenate((python_reviews, python_reviews_two))\n",
    "        py_df = pd.DataFrame(python_reviews_one_and_two)\n",
    "        py_df.columns = ['Python_User_Reviews']\n",
    "        return py_df.Python_User_Reviews.describe()\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        r_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings = map(lambda item: item.text , r_pg_one_rating_tags)\n",
    "        r_user_rating_strings = filter(lambda item: 'out of' in item, r_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053380&spIA=1785283529,1784391034'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Ar+programming&page=2&keywords=r+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        r_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings_two = map(lambda item: item.text , r_pg_two_rating_tags)\n",
    "        r_user_rating_strings_two = filter(lambda item: 'out of' in item, r_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings_two)\n",
    "        r_reviews_one_and_two = np.concatenate((r_reviews, r_reviews_two))\n",
    "        r_df = pd.DataFrame(r_reviews_one_and_two)\n",
    "        r_df.columns = ['R_User_Reviews']\n",
    "        return r_df.R_User_Reviews.describe()\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sas_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings = map(lambda item: item.text , sas_pg_one_rating_tags)\n",
    "        sas_user_rating_strings = filter(lambda item: 'out of' in item, sas_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053426'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asas+programming&page=2&keywords=sas+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sas_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings_two = map(lambda item: item.text , sas_pg_two_rating_tags)\n",
    "        sas_user_rating_strings_two = filter(lambda item: 'out of' in item, sas_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings_two)\n",
    "        sas_reviews_one_and_two = np.concatenate((sas_reviews, sas_reviews_two))\n",
    "        sas_df = pd.DataFrame(sas_reviews_one_and_two)\n",
    "        sas_df.columns = ['SAS_User_Reviews']\n",
    "        return sas_df.SAS_User_Reviews.describe()\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        stata_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings = map(lambda item: item.text , stata_pg_one_rating_tags)\n",
    "        stata_user_rating_strings = filter(lambda item: 'out of' in item, stata_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053462&spIA=1785288148,1783981962'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Astata&page=2&keywords=stata'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        stata_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings_two = map(lambda item: item.text , stata_pg_two_rating_tags)\n",
    "        stata_user_rating_strings_two = filter(lambda item: 'out of' in item, stata_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings_two)\n",
    "        stata_reviews_one_and_two = np.concatenate((stata_reviews, stata_reviews_two))\n",
    "        stata_df = pd.DataFrame(stata_reviews_one_and_two)\n",
    "        stata_df.columns = ['STATA_User_Reviews']\n",
    "        return stata_df.STATA_User_Reviews.describe()\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sql_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings = map(lambda item: item.text , sql_pg_one_rating_tags)\n",
    "        sql_user_rating_strings = filter(lambda item: 'out of' in item, sql_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053498&spIA=B01GK955A4,1782173455'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asql&page=2&keywords=sql'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sql_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings_two = map(lambda item: item.text , sql_pg_two_rating_tags)\n",
    "        sql_user_rating_strings_two = filter(lambda item: 'out of' in item, sql_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings_two)\n",
    "        sql_reviews_one_and_two = np.concatenate((sql_reviews, sql_reviews_two))\n",
    "        sql_df = pd.DataFrame(sql_reviews_one_and_two)\n",
    "        sql_df.columns = ['SQL_User_Reviews']\n",
    "        return sql_df.SQL_User_Reviews.describe()\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        wd_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings = map(lambda item: item.text , wd_pg_one_rating_tags)\n",
    "        wd_user_rating_strings = filter(lambda item: 'out of' in item, wd_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053541&spIA=B01FGXTDGW'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Aweb+development&page=2&keywords=web+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        wd_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings_two = map(lambda item: item.text , wd_pg_two_rating_tags)\n",
    "        wd_user_rating_strings_two = filter(lambda item: 'out of' in item, wd_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings_two)\n",
    "        wd_reviews_one_and_two = np.concatenate((wd_reviews, wd_reviews_two))\n",
    "        wd_df = pd.DataFrame(wd_reviews_one_and_two)\n",
    "        wd_df.columns = ['Web_Dev_User_Reviews']\n",
    "        return wd_df.Web_Dev_User_Reviews.describe()\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        vr_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings = map(lambda item: item.text , vr_pg_one_rating_tags)\n",
    "        vr_user_rating_strings = filter(lambda item: 'out of' in item, vr_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053592&ajr=1'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Avirtual+reality+development&page=2&keywords=virtual+reality+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        vr_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings_two = map(lambda item: item.text , vr_pg_two_rating_tags)\n",
    "        vr_user_rating_strings_two = filter(lambda item: 'out of' in item, vr_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings_two)\n",
    "        vr_reviews_one_and_two = np.concatenate((vr_reviews, vr_reviews_two))\n",
    "        vr_df = pd.DataFrame(vr_reviews_one_and_two)\n",
    "        vr_df.columns = ['VR_User_Reviews']\n",
    "        return vr_df.VR_User_Reviews.describe()\n",
    "    else:\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        da_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings = map(lambda item: item.text , da_pg_one_rating_tags)\n",
    "        da_user_rating_strings = filter(lambda item: 'out of' in item, da_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478123303&spIA=1784390860,B019N212NE'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+analysis&page=2&keywords=data+analysis'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        da_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings_two = map(lambda item: item.text , da_pg_two_rating_tags)\n",
    "        da_user_rating_strings_two = filter(lambda item: 'out of' in item, da_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings_two)\n",
    "        data_analysis_reviews_one_and_two = np.concatenate((data_analysis_reviews, data_analysis_reviews_two))\n",
    "        da_df = pd.DataFrame(data_analysis_reviews_one_and_two)\n",
    "        da_df.columns = ['Data_Analysis_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dv_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings = map(lambda item: item.text , dv_pg_one_rating_tags)\n",
    "        dv_user_rating_strings = filter(lambda item: 'out of' in item, dv_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053109&spIA=1784395803,1784391603'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+visualization&page=2&keywords=data+visualization'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dv_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings_two = map(lambda item: item.text , dv_pg_two_rating_tags)\n",
    "        dv_user_rating_strings_two = filter(lambda item: 'out of' in item, dv_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings_two)\n",
    "        data_viz_reviews_one_and_two = np.concatenate((data_viz_reviews, data_viz_reviews_two))\n",
    "        dv_df = pd.DataFrame(data_viz_reviews_one_and_two)\n",
    "        dv_df.columns = ['Data_Viz_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dm_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings = map(lambda item: item.text , dm_pg_one_rating_tags)\n",
    "        dm_user_rating_strings = filter(lambda item: 'out of' in item, dm_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053297'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+management&page=2&keywords=data+management'+page,headers=headers)  \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dm_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings_two = map(lambda item: item.text , dm_pg_two_rating_tags)\n",
    "        dm_user_rating_strings_two = filter(lambda item: 'out of' in item, dm_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings_two)\n",
    "        data_mgmt_reviews_one_and_two = np.concatenate((data_mgmt_reviews, data_mgmt_reviews_two))\n",
    "        dm_df = pd.DataFrame(data_mgmt_reviews_one_and_two)\n",
    "        dm_df.columns = ['Data_Mgmt_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        py_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings = map(lambda item: item.text , py_pg_one_rating_tags)\n",
    "        py_user_rating_strings = filter(lambda item: 'out of' in item, py_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053333&spIA=B01M63XMN1,B00WFP9S2E'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Apython&page=2&keywords=python'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        py_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings_two = map(lambda item: item.text , py_pg_two_rating_tags)\n",
    "        py_user_rating_strings_two = filter(lambda item: 'out of' in item, py_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings_two)\n",
    "        python_reviews_one_and_two = np.concatenate((python_reviews, python_reviews_two))\n",
    "        py_df = pd.DataFrame(python_reviews_one_and_two)\n",
    "        py_df.columns = ['Python_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        r_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings = map(lambda item: item.text , r_pg_one_rating_tags)\n",
    "        r_user_rating_strings = filter(lambda item: 'out of' in item, r_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053380&spIA=1785283529,1784391034'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Ar+programming&page=2&keywords=r+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        r_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings_two = map(lambda item: item.text , r_pg_two_rating_tags)\n",
    "        r_user_rating_strings_two = filter(lambda item: 'out of' in item, r_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings_two)\n",
    "        r_reviews_one_and_two = np.concatenate((r_reviews, r_reviews_two))\n",
    "        r_df = pd.DataFrame(r_reviews_one_and_two)\n",
    "        r_df.columns = ['R_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sas_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings = map(lambda item: item.text , sas_pg_one_rating_tags)\n",
    "        sas_user_rating_strings = filter(lambda item: 'out of' in item, sas_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053426'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asas+programming&page=2&keywords=sas+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sas_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings_two = map(lambda item: item.text , sas_pg_two_rating_tags)\n",
    "        sas_user_rating_strings_two = filter(lambda item: 'out of' in item, sas_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings_two)\n",
    "        sas_reviews_one_and_two = np.concatenate((sas_reviews, sas_reviews_two))\n",
    "        sas_df = pd.DataFrame(sas_reviews_one_and_two)\n",
    "        sas_df.columns = ['SAS_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        stata_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings = map(lambda item: item.text , stata_pg_one_rating_tags)\n",
    "        stata_user_rating_strings = filter(lambda item: 'out of' in item, stata_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053462&spIA=1785288148,1783981962'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Astata&page=2&keywords=stata'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        stata_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings_two = map(lambda item: item.text , stata_pg_two_rating_tags)\n",
    "        stata_user_rating_strings_two = filter(lambda item: 'out of' in item, stata_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings_two)\n",
    "        stata_reviews_one_and_two = np.concatenate((stata_reviews, stata_reviews_two))\n",
    "        stata_df = pd.DataFrame(stata_reviews_one_and_two)\n",
    "        stata_df.columns = ['STATA_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sql_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings = map(lambda item: item.text , sql_pg_one_rating_tags)\n",
    "        sql_user_rating_strings = filter(lambda item: 'out of' in item, sql_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053498&spIA=B01GK955A4,1782173455'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asql&page=2&keywords=sql'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sql_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings_two = map(lambda item: item.text , sql_pg_two_rating_tags)\n",
    "        sql_user_rating_strings_two = filter(lambda item: 'out of' in item, sql_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings_two)\n",
    "        sql_reviews_one_and_two = np.concatenate((sql_reviews, sql_reviews_two))\n",
    "        sql_df = pd.DataFrame(sql_reviews_one_and_two)\n",
    "        sql_df.columns = ['SQL_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        wd_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings = map(lambda item: item.text , wd_pg_one_rating_tags)\n",
    "        wd_user_rating_strings = filter(lambda item: 'out of' in item, wd_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053541&spIA=B01FGXTDGW'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Aweb+development&page=2&keywords=web+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        wd_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings_two = map(lambda item: item.text , wd_pg_two_rating_tags)\n",
    "        wd_user_rating_strings_two = filter(lambda item: 'out of' in item, wd_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings_two)\n",
    "        wd_reviews_one_and_two = np.concatenate((wd_reviews, wd_reviews_two))\n",
    "        wd_df = pd.DataFrame(wd_reviews_one_and_two)\n",
    "        wd_df.columns = ['Web_Dev_User_Reviews']\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        vr_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings = map(lambda item: item.text , vr_pg_one_rating_tags)\n",
    "        vr_user_rating_strings = filter(lambda item: 'out of' in item, vr_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053592&ajr=1'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Avirtual+reality+development&page=2&keywords=virtual+reality+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        vr_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings_two = map(lambda item: item.text , vr_pg_two_rating_tags)\n",
    "        vr_user_rating_strings_two = filter(lambda item: 'out of' in item, vr_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings_two)\n",
    "        vr_reviews_one_and_two = np.concatenate((vr_reviews, vr_reviews_two))\n",
    "        vr_df = pd.DataFrame(vr_reviews_one_and_two)\n",
    "        vr_df.columns = ['VR_User_Reviews']\n",
    "        user_ratings = pd.concat([ds_df, da_df , dv_df, dm_df, py_df, r_df, sas_df, stata_df, sql_df, wd_df, vr_df], axis=1)\n",
    "        print ('If you want a specific subject area, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')\n",
    "        return user_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want a specific subject area, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data_Science_User_Reviews</th>\n",
       "      <th>Data_Analysis_User_Reviews</th>\n",
       "      <th>Data_Viz_User_Reviews</th>\n",
       "      <th>Data_Mgmt_User_Reviews</th>\n",
       "      <th>Python_User_Reviews</th>\n",
       "      <th>R_User_Reviews</th>\n",
       "      <th>SAS_User_Reviews</th>\n",
       "      <th>STATA_User_Reviews</th>\n",
       "      <th>SQL_User_Reviews</th>\n",
       "      <th>Web_Dev_User_Reviews</th>\n",
       "      <th>VR_User_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.336667</td>\n",
       "      <td>4.296774</td>\n",
       "      <td>4.360000</td>\n",
       "      <td>4.426923</td>\n",
       "      <td>4.387879</td>\n",
       "      <td>4.348485</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>4.316000</td>\n",
       "      <td>4.325714</td>\n",
       "      <td>4.262500</td>\n",
       "      <td>4.205263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.347884</td>\n",
       "      <td>0.484757</td>\n",
       "      <td>0.440689</td>\n",
       "      <td>0.446818</td>\n",
       "      <td>0.327641</td>\n",
       "      <td>0.460381</td>\n",
       "      <td>0.579032</td>\n",
       "      <td>0.808125</td>\n",
       "      <td>0.603101</td>\n",
       "      <td>0.801913</td>\n",
       "      <td>0.659523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Data_Science_User_Reviews  Data_Analysis_User_Reviews  \\\n",
       "count                  30.000000                   31.000000   \n",
       "mean                    4.336667                    4.296774   \n",
       "std                     0.347884                    0.484757   \n",
       "min                     3.500000                    3.100000   \n",
       "25%                     4.200000                    4.050000   \n",
       "50%                     4.300000                    4.400000   \n",
       "75%                     4.600000                    4.600000   \n",
       "max                     5.000000                    5.000000   \n",
       "\n",
       "       Data_Viz_User_Reviews  Data_Mgmt_User_Reviews  Python_User_Reviews  \\\n",
       "count              30.000000               26.000000            33.000000   \n",
       "mean                4.360000                4.426923             4.387879   \n",
       "std                 0.440689                0.446818             0.327641   \n",
       "min                 3.000000                3.400000             3.500000   \n",
       "25%                 4.300000                4.225000             4.200000   \n",
       "50%                 4.400000                4.450000             4.400000   \n",
       "75%                 4.600000                4.750000             4.600000   \n",
       "max                 5.000000                5.000000             4.900000   \n",
       "\n",
       "       R_User_Reviews  SAS_User_Reviews  STATA_User_Reviews  SQL_User_Reviews  \\\n",
       "count       33.000000         28.000000           25.000000         35.000000   \n",
       "mean         4.348485          4.225000            4.316000          4.325714   \n",
       "std          0.460381          0.579032            0.808125          0.603101   \n",
       "min          3.000000          3.000000            1.500000          1.300000   \n",
       "25%          4.100000          4.000000            4.200000          4.200000   \n",
       "50%          4.400000          4.250000            4.600000          4.400000   \n",
       "75%          4.700000          4.700000            4.800000          4.600000   \n",
       "max          5.000000          5.000000            5.000000          5.000000   \n",
       "\n",
       "       Web_Dev_User_Reviews  VR_User_Reviews  \n",
       "count             32.000000        19.000000  \n",
       "mean               4.262500         4.205263  \n",
       "std                0.801913         0.659523  \n",
       "min                1.000000         2.600000  \n",
       "25%                4.100000         4.000000  \n",
       "50%                4.450000         4.300000  \n",
       "75%                4.700000         4.500000  \n",
       "max                5.000000         5.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryStats('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valueCounts(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return  ds_df.Data_Science_User_Reviews.value_counts()\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        da_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings = map(lambda item: item.text , da_pg_one_rating_tags)\n",
    "        da_user_rating_strings = filter(lambda item: 'out of' in item, da_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478123303&spIA=1784390860,B019N212NE'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+analysis&page=2&keywords=data+analysis'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        da_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        da_rating_strings_two = map(lambda item: item.text , da_pg_two_rating_tags)\n",
    "        da_user_rating_strings_two = filter(lambda item: 'out of' in item, da_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_analysis_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), da_user_rating_strings_two)\n",
    "        data_analysis_reviews_one_and_two = np.concatenate((data_analysis_reviews, data_analysis_reviews_two))\n",
    "        da_df = pd.DataFrame(data_analysis_reviews_one_and_two)\n",
    "        da_df.columns = ['Data_Analysis_User_Reviews']\n",
    "        return da_df.Data_Analysis_User_Reviews.value_counts()\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dv_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings = map(lambda item: item.text , dv_pg_one_rating_tags)\n",
    "        dv_user_rating_strings = filter(lambda item: 'out of' in item, dv_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053109&spIA=1784395803,1784391603'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+visualization&page=2&keywords=data+visualization'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dv_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dv_rating_strings_two = map(lambda item: item.text , dv_pg_two_rating_tags)\n",
    "        dv_user_rating_strings_two = filter(lambda item: 'out of' in item, dv_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_viz_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dv_user_rating_strings_two)\n",
    "        data_viz_reviews_one_and_two = np.concatenate((data_viz_reviews, data_viz_reviews_two))\n",
    "        dv_df = pd.DataFrame(data_viz_reviews_one_and_two)\n",
    "        dv_df.columns = ['Data_Viz_User_Reviews']\n",
    "        return dv_df.Data_Viz_User_Reviews.value_counts()\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        dm_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings = map(lambda item: item.text , dm_pg_one_rating_tags)\n",
    "        dm_user_rating_strings = filter(lambda item: 'out of' in item, dm_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053297'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+management&page=2&keywords=data+management'+page,headers=headers)  \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        dm_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        dm_rating_strings_two = map(lambda item: item.text , dm_pg_two_rating_tags)\n",
    "        dm_user_rating_strings_two = filter(lambda item: 'out of' in item, dm_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_mgmt_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), dm_user_rating_strings_two)\n",
    "        data_mgmt_reviews_one_and_two = np.concatenate((data_mgmt_reviews, data_mgmt_reviews_two))\n",
    "        dm_df = pd.DataFrame(data_mgmt_reviews_one_and_two)\n",
    "        dm_df.columns = ['Data_Mgmt_User_Reviews']\n",
    "        return dm_df.Data_Mgmt_User_Reviews.value_counts()\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        py_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings = map(lambda item: item.text , py_pg_one_rating_tags)\n",
    "        py_user_rating_strings = filter(lambda item: 'out of' in item, py_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053333&spIA=B01M63XMN1,B00WFP9S2E'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Apython&page=2&keywords=python'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        py_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        py_rating_strings_two = map(lambda item: item.text , py_pg_two_rating_tags)\n",
    "        py_user_rating_strings_two = filter(lambda item: 'out of' in item, py_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        python_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), py_user_rating_strings_two)\n",
    "        python_reviews_one_and_two = np.concatenate((python_reviews, python_reviews_two))\n",
    "        py_df = pd.DataFrame(python_reviews_one_and_two)\n",
    "        py_df.columns = ['Python_User_Reviews']\n",
    "        return py_df.Python_User_Reviews.value_counts()\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        r_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings = map(lambda item: item.text , r_pg_one_rating_tags)\n",
    "        r_user_rating_strings = filter(lambda item: 'out of' in item, r_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053380&spIA=1785283529,1784391034'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Ar+programming&page=2&keywords=r+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        r_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        r_rating_strings_two = map(lambda item: item.text , r_pg_two_rating_tags)\n",
    "        r_user_rating_strings_two = filter(lambda item: 'out of' in item, r_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        r_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), r_user_rating_strings_two)\n",
    "        r_reviews_one_and_two = np.concatenate((r_reviews, r_reviews_two))\n",
    "        r_df = pd.DataFrame(r_reviews_one_and_two)\n",
    "        r_df.columns = ['R_User_Reviews']\n",
    "        return r_df.R_User_Reviews.value_counts()\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sas_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings = map(lambda item: item.text , sas_pg_one_rating_tags)\n",
    "        sas_user_rating_strings = filter(lambda item: 'out of' in item, sas_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053426'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asas+programming&page=2&keywords=sas+programming'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sas_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sas_rating_strings_two = map(lambda item: item.text , sas_pg_two_rating_tags)\n",
    "        sas_user_rating_strings_two = filter(lambda item: 'out of' in item, sas_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sas_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sas_user_rating_strings_two)\n",
    "        sas_reviews_one_and_two = np.concatenate((sas_reviews, sas_reviews_two))\n",
    "        sas_df = pd.DataFrame(sas_reviews_one_and_two)\n",
    "        sas_df.columns = ['SAS_User_Reviews']\n",
    "        return sas_df.SAS_User_Reviews.value_counts()\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        stata_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings = map(lambda item: item.text , stata_pg_one_rating_tags)\n",
    "        stata_user_rating_strings = filter(lambda item: 'out of' in item, stata_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053462&spIA=1785288148,1783981962'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Astata&page=2&keywords=stata'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        stata_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        stata_rating_strings_two = map(lambda item: item.text , stata_pg_two_rating_tags)\n",
    "        stata_user_rating_strings_two = filter(lambda item: 'out of' in item, stata_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        stata_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), stata_user_rating_strings_two)\n",
    "        stata_reviews_one_and_two = np.concatenate((stata_reviews, stata_reviews_two))\n",
    "        stata_df = pd.DataFrame(stata_reviews_one_and_two)\n",
    "        stata_df.columns = ['STATA_User_Reviews']\n",
    "        return stata_df.STATA_User_Reviews.value_counts()\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        sql_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings = map(lambda item: item.text , sql_pg_one_rating_tags)\n",
    "        sql_user_rating_strings = filter(lambda item: 'out of' in item, sql_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053498&spIA=B01GK955A4,1782173455'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Asql&page=2&keywords=sql'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        sql_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        sql_rating_strings_two = map(lambda item: item.text , sql_pg_two_rating_tags)\n",
    "        sql_user_rating_strings_two = filter(lambda item: 'out of' in item, sql_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        sql_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), sql_user_rating_strings_two)\n",
    "        sql_reviews_one_and_two = np.concatenate((sql_reviews, sql_reviews_two))\n",
    "        sql_df = pd.DataFrame(sql_reviews_one_and_two)\n",
    "        sql_df.columns = ['SQL_User_Reviews']\n",
    "        return sql_df.SQL_User_Reviews.value_counts()\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        wd_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings = map(lambda item: item.text , wd_pg_one_rating_tags)\n",
    "        wd_user_rating_strings = filter(lambda item: 'out of' in item, wd_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053541&spIA=B01FGXTDGW'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Aweb+development&page=2&keywords=web+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        wd_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        wd_rating_strings_two = map(lambda item: item.text , wd_pg_two_rating_tags)\n",
    "        wd_user_rating_strings_two = filter(lambda item: 'out of' in item, wd_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        wd_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), wd_user_rating_strings_two)\n",
    "        wd_reviews_one_and_two = np.concatenate((wd_reviews, wd_reviews_two))\n",
    "        wd_df = pd.DataFrame(wd_reviews_one_and_two)\n",
    "        wd_df.columns = ['Web_Dev_User_Reviews']\n",
    "        return wd_df.Web_Dev_User_Reviews.value_counts()\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        vr_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings = map(lambda item: item.text , vr_pg_one_rating_tags)\n",
    "        vr_user_rating_strings = filter(lambda item: 'out of' in item, vr_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1479053592&ajr=1'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Avirtual+reality+development&page=2&keywords=virtual+reality+development'+page,headers=headers) \n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        vr_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        vr_rating_strings_two = map(lambda item: item.text , vr_pg_two_rating_tags)\n",
    "        vr_user_rating_strings_two = filter(lambda item: 'out of' in item, vr_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        vr_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), vr_user_rating_strings_two)\n",
    "        vr_reviews_one_and_two = np.concatenate((vr_reviews, vr_reviews_two))\n",
    "        vr_df = pd.DataFrame(vr_reviews_one_and_two)\n",
    "        vr_df.columns = ['VR_User_Reviews']\n",
    "        return vr_df.VR_User_Reviews.value_counts()\n",
    "    else:\n",
    "       print ('If you want a specific subject area, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3    8\n",
       "4.2    5\n",
       "4.7    4\n",
       "4.6    3\n",
       "4.0    3\n",
       "4.8    2\n",
       "3.5    2\n",
       "4.1    1\n",
       "4.4    1\n",
       "5.0    1\n",
       "Name: Data_Science_User_Reviews, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valueCounts('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregatedBarChart(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return  ds_df.Data_Science_User_Reviews.value_counts().plot(kind = 'bar')\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117031290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFsCAYAAADYP1DdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f36f610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aggregatedBarChart('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def barChart(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return  ds_df.Data_Science_User_Reviews.plot(kind = 'bar')\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e41dcd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFoCAYAAABDrhLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuSRJhTPMJUkq3P8HcQjw0HECMpAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e41d690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "barChart('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histChart(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        return  ds_df.Data_Science_User_Reviews.plot(kind = 'hist')\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x122404210>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFkCAYAAACabLnAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHRIZIE4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12241dc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histChart('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boxChart(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_rating_tags = b1.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings = map(lambda item: item.text , ds_pg_one_rating_tags)\n",
    "        ds_user_rating_strings = filter(lambda item: 'out of' in item, ds_rating_strings)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_rating_tags = b2.findAll('span', attrs={'class':'a-icon-alt'})\n",
    "        ds_rating_strings_two = map(lambda item: item.text , ds_pg_two_rating_tags)\n",
    "        ds_user_rating_strings_two = filter(lambda item: 'out of' in item, ds_rating_strings_two)\n",
    "        rating_pattern = re.compile(\"\\d+(\\.\\d)*\")\n",
    "        data_science_reviews_two = map(lambda item: float(re.search(rating_pattern, item).group(0)), ds_user_rating_strings_two)\n",
    "        data_science_reviews_one_and_two = np.concatenate((data_science_reviews, data_science_reviews_two))\n",
    "        ds_df = pd.DataFrame(data_science_reviews_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_User_Reviews']\n",
    "        ds_df.Data_Science_User_Reviews.plot(kind = 'box')\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAFkCAYAAABW9YMrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ab7e050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxChart('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_title_tags = b1.findAll('a', attrs={'class':'a-link-normal s-access-detail-page a-text-normal'})\n",
    "        ds_title_strings = map(lambda item: item.text , ds_pg_one_title_tags)\n",
    "        title_pattern = re.compile(\".+\")\n",
    "        data_science_titles = map(lambda item: re.search(title_pattern, item).group(0), ds_title_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_title_tags = b2.findAll('a', attrs={'class':'a-link-normal s-access-detail-page a-text-normal'})\n",
    "        ds_title_strings_two = map(lambda item: item.text , ds_pg_two_title_tags)\n",
    "        title_pattern = re.compile(\".+\")\n",
    "        data_science_titles_two = map(lambda item: re.search(title_pattern, item).group(0), ds_title_strings_two)\n",
    "        data_science_book_titles_one_and_two = np.concatenate((data_science_titles, data_science_titles_two))\n",
    "        ds_titles_df = pd.DataFrame(data_science_book_titles_one_and_two)\n",
    "        ds_titles_df.columns = ['Data_Science_Book_Titles']\n",
    "        return ds_titles_df\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ab1e2007035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "title('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_price_tags = b1.findAll('span', attrs={'class':'a-size-base a-color-price s-price a-text-bold'})\n",
    "        ds_price_strings = map(lambda item: item.text , ds_pg_one_price_tags)\n",
    "        ds_amazon_price_strings = filter(lambda item: 'out of' in item, ds_price_strings)\n",
    "        price_pattern = re.compile(\"\\d+\\.\\d+\")\n",
    "        data_science_prices = map(lambda item: float(re.search(price_pattern, item).group(0)), ds_amazon_price_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_price_tags = b2.findAll('span', attrs={'class':'a-size-base a-color-price s-price a-text-bold'})\n",
    "        ds_price_strings_two = map(lambda item: item.text , ds_pg_two_price_tags)\n",
    "        price_pattern = re.compile(\"\\d+\\.\\d+\")\n",
    "        data_science_prices_two = map(lambda item: float(re.search(price_pattern, item).group(0)), ds_price_strings_two)\n",
    "        data_science_prices_one_and_two = np.concatenate((data_science_prices, data_science_prices_two))\n",
    "        ds_prices_df = pd.DataFrame(data_science_prices_one_and_two)\n",
    "        ds_prices_df.columns = ['Data_Science_Book_Prices']\n",
    "        return ds_prices_df\n",
    "    elif subject == 'data analysis':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+analysis&rh=i%3Aaps%2Ck%3Adata+analysis', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data visualization' or subject == 'data viz':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+visualization',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'data management':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+management&rh=i%3Aaps%2Ck%3Adata+management',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'python':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=python&rh=i%3Aaps%2Ck%3Apython',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'r':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming&rh=i%3Aaps%2Ck%3Ar+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sas':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sas+programming&rh=i%3Aaps%2Ck%3Asas+programming',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'stata':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=stata&rh=i%3Aaps%2Ck%3Astata',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'sql':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=sql&rh=i%3Aaps%2Ck%3Asql',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'web developmet' or subject == 'web dev':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=web+development&rh=i%3Aaps%2Ck%3Aweb+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    elif subject == 'virtual reality' or subject == 'gaming' or subject == 'virtual reality development':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=virtual+reality+development&rh=i%3Aaps%2Ck%3Avirtual+reality+development',headers=headers) \n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    else:\n",
    "        print ('Sorry, please choose from the following topics: data science, data analysis, data visualization, data management, sql, python, r, sas, stata, web development, or virtual reality development.:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data_Science_Book_Prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>74.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>63.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>42.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data_Science_Book_Prices\n",
       "0                      37.99\n",
       "1                      64.04\n",
       "2                      22.18\n",
       "3                      49.14\n",
       "4                      39.59\n",
       "5                      24.99\n",
       "6                      53.61\n",
       "7                      20.00\n",
       "8                      40.74\n",
       "9                      49.54\n",
       "10                     33.99\n",
       "11                      0.00\n",
       "12                      0.00\n",
       "13                     10.99\n",
       "14                     74.00\n",
       "15                     63.99\n",
       "16                      0.00\n",
       "17                      2.99\n",
       "18                     13.38\n",
       "19                     13.30\n",
       "20                     13.99\n",
       "21                      0.00\n",
       "22                     42.31\n",
       "23                     32.19\n",
       "24                      7.95\n",
       "25                     17.99\n",
       "26                      2.99\n",
       "27                     23.86\n",
       "28                     22.67\n",
       "29                     34.99\n",
       "30                     44.99"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def author(topic):\n",
    "    subject = topic.lower()\n",
    "    if subject == 'data science':\n",
    "        r1 = requests.get('https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=data+science', headers=headers)\n",
    "        b1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "        ds_pg_one_author_tags = b1.findAll('a', attrs={'class':'a-link-normal a-text-normal'})\n",
    "        ds_author_strings = map(lambda item: item.text , ds_pg_one_author_tags)\n",
    "        ds_author_strings = filter(lambda item: 'by' in item, ds_author_strings)\n",
    "        author_pattern = re.compile(\"\\w+\")\n",
    "        data_science_authors = map(lambda item: re.search(author_pattern, item).group(0), ds_author_strings)\n",
    "        page = '&ie=UTF8&qid=1478029914&spIA=1783982047,1783553359'\n",
    "        r2 = requests.get('https://www.amazon.com/s/ref=sr_pg_2?rh=i%3Aaps%2Ck%3Adata+science&page=2&keywords=data+science'+page,headers=headers)\n",
    "        b2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "        ds_pg_two_author_tags = b2.findAll('a', attrs={'class':'a-link-normal a-text-normal'})\n",
    "        ds_author_strings_two = map(lambda item: item.text , ds_pg_two_author_tags)\n",
    "        ds_author_strings = filter(lambda item: 'by' in item, ds_author_strings)\n",
    "        author_pattern = re.compile(\"\\w+\")\n",
    "        data_science_author_two = map(lambda item: re.search(author_pattern, item).group(0), ds_author_strings_two)\n",
    "        data_science_author_one_and_two = np.concatenate((data_science_authors, data_science_author_two))\n",
    "        ds_df = pd.DataFrame(data_science_author_one_and_two)\n",
    "        ds_df.columns = ['Data_Science_Authors']\n",
    "        return ds_df\n",
    "    else:\n",
    "        print ('What the heck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b55d1fe73955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauthor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data science'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-9fa8b1f8ff54>\u001b[0m in \u001b[0;36mauthor\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mds_author_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'by'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_author_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mauthor_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdata_science_author_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_author_strings_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdata_science_author_one_and_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_science_authors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_science_author_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mds_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_science_author_one_and_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-9fa8b1f8ff54>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mds_author_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'by'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_author_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mauthor_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdata_science_author_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_author_strings_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdata_science_author_one_and_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_science_authors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_science_author_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mds_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_science_author_one_and_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "author('data science')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:elledubby]",
   "language": "python",
   "name": "conda-env-elledubby-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
